{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 67886,
     "status": "ok",
     "timestamp": 1604882799322,
     "user": {
      "displayName": "Xin Yao",
      "photoUrl": "",
      "userId": "04992890314986629859"
     },
     "user_tz": -480
    },
    "id": "JTstRklJbFYJ",
    "outputId": "17b50471-d834-4ae9-e618-929d76573123"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import mldata\n",
    "import util\n",
    "import numpy as np\n",
    "import preprocess\n",
    "# import previous.dtree\n",
    "import previous.logreg as logreg\n",
    "import previous.nbayes as nbayes\n",
    "from previous.ID3_dtree import ID3DecisionTree \n",
    "import sys\n",
    "\n",
    "# Option 1 is the path to the data as in previous problems.\n",
    "# Option 2 is a 0/1 option: \n",
    "# If 0, use cross validation. \n",
    "# If 1, run the algorithm on the full sample. \n",
    "# Option 3 is a learning algorithm. \n",
    "# This can be dtree, nbayes or logreg. \n",
    "# For each value, the base classifier will be the corresponding algorithm you have implemented in your previous assignments. \n",
    "# You should use the decision stumps (trees of depth 1, containing just the root), for the dtree option. \n",
    "# All parameters for these base classifiers should be “reasonable” values, i.e. use\n",
    "# values that you have previously found to work well with these algorithms.\n",
    "# Option 4 is the number of iterations to perform boosting\n",
    "\n",
    "# Remember that for boosting, you will need to modify your previous code so it works with weighted\n",
    "# examples. Some of these modifications were described in class.\n",
    "\n",
    "def create_for_train(X_data, y_data, folds, n_bin, index):\n",
    "    if n_bin == 1:\n",
    "        return X_data, y_data, X_data, y_data\n",
    "    else:\n",
    "        x_train = []\n",
    "        y_train = []\n",
    "        x_test = folds[index]\n",
    "        y_test = folds[index]\n",
    "        for i in range(n_bin):\n",
    "            if i == index:\n",
    "                continue\n",
    "            x_train.extend(folds[i])\n",
    "            y_train.extend(folds[i])\n",
    "\n",
    "        return X_data[x_train, :], y_data[y_train], X_data[x_test, :], y_data[y_test]\n",
    "\n",
    "def boost(path, option, solver_type, num_iters): \n",
    "    path = path.replace(\"\\\\\", \"/\")\n",
    "    file_base = path.split('/')[-1]\n",
    "    rootdir=path\n",
    "\n",
    "    epsilon_thread = 0.00000001\n",
    "\n",
    "    data = mldata.parse_c45(file_base, rootdir)\n",
    "    n_bin = 1\n",
    "    cross_validation = False\n",
    "    if option == 0:\n",
    "        n_bin = 5\n",
    "        cross_validation = True\n",
    "\n",
    "    data = np.asarray(data.to_float())\n",
    "    X_data = data[:, 1:-1]\n",
    "    X_data = preprocess.process(X_data, file_base, n_bin)\n",
    "    y_data = data[:, -1].astype(int)\n",
    "    # print(len(X_data))\n",
    "    # partition the data into multiple dataset,\n",
    "    folds = util.n_fold(len(data), n_bin)\n",
    "\n",
    "    # nbayes:\n",
    "    posi_num = [{} for i in range(len(X_data[0]))]\n",
    "    nega_num = [{} for i in range(len(X_data[0]))]\n",
    "            \n",
    "    for i, d in enumerate(posi_num):\n",
    "        for attr in np.unique(X_data[:, i]):\n",
    "            posi_num[i][attr] = 0\n",
    "                    \n",
    "    for i, d in enumerate(nega_num):\n",
    "        for attr in np.unique(X_data[:, i]):\n",
    "            nega_num[i][attr] = 0\n",
    "\n",
    "    AUC_y = []\n",
    "    pred_AUC_y = []\n",
    "    acc = []\n",
    "    prec = []\n",
    "    rec = []\n",
    "    # training and evaluating\n",
    "                \n",
    "    for i in range(n_bin):\n",
    "        if solver_type == \"dtree\":\n",
    "            tree = ID3DecisionTree(1, path, \"gain\", cross_validation)\n",
    "            x_train, y_train, x_test, y_test = tree.create_for_train(n_bin, i)\n",
    "            train_size = len(x_train)\n",
    "            wboost = np.ones((train_size, 1)).astype(float)/train_size\n",
    "            alphas = []\n",
    "            epsilons = []\n",
    "            forest = []\n",
    "            for iter_ in range(num_iters): \n",
    "                tree = ID3DecisionTree(1, path, \"gain\", cross_validation)\n",
    "                x_train, y_train, x_test, y_test = tree.create_for_train(n_bin, i)\n",
    "                D_train = (x_train, y_train)\n",
    "                wboost, epsilon, alpha = tree.boosttrain(D_train, wboost, epsilon_thread)\n",
    "                forest.append(tree)\n",
    "                epsilons.append(epsilon)\n",
    "                if epsilon == 0:\n",
    "                    alphas = [0] * len(alphas)\n",
    "                    alphas.append(1)\n",
    "                    break\n",
    "                elif epsilon <= epsilon_thread or epsilon >= 0.5:\n",
    "                    alphas.append(alpha)\n",
    "                    break\n",
    "                else:\n",
    "                    alphas.append(alpha)\n",
    "                # y_pred = tree.test(x_test)\n",
    "            result = []\n",
    "            for i in range(len(forest)):\n",
    "                y_predB = forest[i].test(x_test)\n",
    "                y_pred = np.array(y_predB)\n",
    "                y_pred[y_pred==False] = 0\n",
    "                y_pred[y_pred==True] = 1\n",
    "                result.append(y_pred)\n",
    "            alphas = np.array(alphas)\n",
    "            alphas = alphas/np.sum(alphas)\n",
    "            y_pred = alphas.dot(np.array(result))\n",
    "            y_pred[y_pred<0.5] = 0\n",
    "            y_pred[y_pred>=0.5] = 1\n",
    "            y_test = np.array(y_test)\n",
    "            y_test[y_test<0.5] = 0\n",
    "            y_test[y_test>=0.5] = 1\n",
    "            AUC_y.extend(y_test)\n",
    "            pred_AUC_y.extend(y_pred)\n",
    "            _acc, _prec, _rec = util.cal_APR(y_pred, y_test)\n",
    "            if cross_validation:\n",
    "                util.report_cross(_acc, _prec, _rec)\n",
    "            acc.append(_acc)\n",
    "            prec.append(_prec)\n",
    "            rec.append(_rec)\n",
    "        elif solver_type == \"nbayes\":\n",
    "            m_etimate = 0.1\n",
    "            x_train, y_train, x_test, y_test = create_for_train(X_data, y_data, folds, n_bin, i)\n",
    "            train_size = len(x_train)\n",
    "            wboost = np.ones((train_size, 1)).astype(float)/train_size\n",
    "            alphas = []\n",
    "            epsilons = []\n",
    "            pre_ps = []\n",
    "            posi_ps = []\n",
    "            nega_ps = []\n",
    "            for iter_ in range(num_iters): \n",
    "                pre_p, posi_p, nega_p, epsilon, alpha, wboost = nbayes.boosttrain_bayes(x_train, y_train, m_etimate, posi_num, nega_num, wboost, epsilon_thread)\n",
    "                epsilons.append(epsilon)\n",
    "                pre_ps.append(pre_p)\n",
    "                posi_ps.append(posi_p)\n",
    "                nega_ps.append(nega_p)\n",
    "                if epsilon == 0:\n",
    "                    alphas = [0] * len(alphas)\n",
    "                    alphas.append(1)\n",
    "                    break\n",
    "                elif epsilon <= epsilon_thread or epsilon >= 0.5:\n",
    "                    alphas.append(alpha)\n",
    "                    break\n",
    "                else:\n",
    "                    alphas.append(alpha)\n",
    "            result = []\n",
    "            for i in range(len(pre_ps)):\n",
    "                y_predB = nbayes.pred(x_test, pre_ps[i], posi_ps[i], nega_ps[i])\n",
    "                y_pred = []\n",
    "                for i in y_predB:\n",
    "                    if i[0] > i[1]:\n",
    "                        y_pred.append(0)\n",
    "                    else:\n",
    "                        y_pred.append(1)\n",
    "                y_pred = np.array(y_pred)\n",
    "                result.append(y_pred)\n",
    "            alphas = np.array(alphas)\n",
    "            alphas = alphas/np.sum(alphas)\n",
    "            y_pred = alphas.dot(np.array(result))\n",
    "            y_pred[y_pred<0.5] = 0\n",
    "            y_pred[y_pred>=0.5] = 1\n",
    "            AUC_y.extend(y_test)\n",
    "            pred_AUC_y.extend(y_pred)\n",
    "            _acc, _prec, _rec = util.cal_APR(y_pred, y_test)\n",
    "            if cross_validation:\n",
    "                util.report_cross(_acc, _prec, _rec)\n",
    "            acc.append(_acc)\n",
    "            prec.append(_prec)\n",
    "            rec.append(_rec)\n",
    "        elif solver_type == \"logreg\":\n",
    "            # train\n",
    "            x_train, y_train, x_test, y_test = create_for_train(X_data, y_data, folds, n_bin, i)\n",
    "            train_size = len(x_train)\n",
    "            wboost = np.ones((train_size, 1)).astype(float)/train_size\n",
    "            alphas = []\n",
    "            epsilons = []\n",
    "            weights = []\n",
    "            for iter_ in range(num_iters): \n",
    "                weight, epsilon, alpha, wboost = logreg.boostLR(x_train, y_train, wboost, epsilon_thread, max_iters=500, lbd=0.1)\n",
    "                epsilons.append(epsilon)\n",
    "                weights.append(weight)\n",
    "                if epsilon == 0:\n",
    "                    alphas = [0] * len(alphas)\n",
    "                    alphas.append(1)\n",
    "                    break\n",
    "                elif epsilon <= epsilon_thread or epsilon >= 0.5:\n",
    "                    alphas.append(alpha)\n",
    "                    break\n",
    "                else:\n",
    "                    alphas.append(alpha)\n",
    "            result = []\n",
    "            for i in range(len(weights)):\n",
    "                result.append(logreg.pred(x_test, weights[i]))\n",
    "            alphas = np.array(alphas)\n",
    "            alphas = alphas/np.sum(alphas)\n",
    "            y_pred = alphas.dot(np.array(result))\n",
    "            y_pred[y_pred<0.5] = 0\n",
    "            y_pred[y_pred>=0.5] = 1\n",
    "            AUC_y.extend(y_test)\n",
    "            pred_AUC_y.extend(y_pred)\n",
    "            _acc, _prec, _rec = logreg.cal_LR_APR(y_pred, y_test)\n",
    "            if cross_validation:\n",
    "                util.report_cross(_acc, _prec, _rec)\n",
    "            acc.append(_acc)\n",
    "            prec.append(_prec)\n",
    "            rec.append(_rec)\n",
    "        else:\n",
    "            return \n",
    "    roc_score = logreg.cal_AUC(AUC_y, pred_AUC_y)\n",
    "    util.report(acc, prec, rec, roc_score)\n",
    "    if type(acc) is list:\n",
    "        return np.mean(acc), np.mean(prec), np.mean(rec), roc_score\n",
    "    return acc, prec, rec, roc_score\n",
    "# When your code is run, it should first construct 5 folds using stratified cross validation if this option is\n",
    "# provided. To ensure repeatability, set the random seed for the PRNG to 12345. Then it should produce\n",
    "# boosted models on each fold (or the sample according to the option)\n",
    "\n",
    "# Accuracy: 0.xyz 0.abc\n",
    "# Precision: 0.xyz 0.abc\n",
    "# Recall: 0.xyz 0.abc\n",
    "# Area under ROC: 0.xyz\n",
    "# “0.xyz” is the average value of each quantity over five folds.\n",
    "# “0.abc” is the standard deviation. \n",
    "# For Area under ROC, use the “pooling” method described in problem 2.\n",
    "# from util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 68031,
     "status": "ok",
     "timestamp": 1604882802520,
     "user": {
      "displayName": "Xin Yao",
      "photoUrl": "",
      "userId": "04992890314986629859"
     },
     "user_tz": -480
    },
    "id": "GAcEGTI7l54V",
    "outputId": "87184f44-a69b-4829-ef30-629b905bcd43",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============Fold report==================\n",
      "Accuracy:0.694\n",
      "Precision:0.692\n",
      "Recall:0.917\n",
      "===============Fold report==================\n",
      "Accuracy:0.628\n",
      "Precision:0.627\n",
      "Recall:0.999\n",
      "===============Fold report==================\n",
      "Accuracy:0.514\n",
      "Precision:0.942\n",
      "Recall:0.246\n",
      "===============Fold report==================\n",
      "Accuracy:0.513\n",
      "Precision:0.962\n",
      "Recall:0.220\n",
      "===============Fold report==================\n",
      "Accuracy:0.629\n",
      "Precision:0.629\n",
      "Recall:1.000\n",
      "===============Final report=================\n",
      "Accuracy:0.596 0.071\n",
      "Precision:0.770 0.150\n",
      "Recall:0.677 0.363\n",
      "Area under ROC 0.074\n",
      "===============Fold report==================\n",
      "Accuracy:0.694\n",
      "Precision:0.692\n",
      "Recall:0.917\n",
      "===============Fold report==================\n",
      "Accuracy:0.628\n",
      "Precision:0.627\n",
      "Recall:0.999\n",
      "===============Fold report==================\n",
      "Accuracy:0.514\n",
      "Precision:0.942\n",
      "Recall:0.246\n",
      "===============Fold report==================\n",
      "Accuracy:0.513\n",
      "Precision:0.962\n",
      "Recall:0.220\n",
      "===============Fold report==================\n",
      "Accuracy:0.629\n",
      "Precision:0.629\n",
      "Recall:1.000\n",
      "===============Final report=================\n",
      "Accuracy:0.596 0.071\n",
      "Precision:0.770 0.150\n",
      "Recall:0.677 0.363\n",
      "Area under ROC 0.074\n",
      "===============Fold report==================\n",
      "Accuracy:0.694\n",
      "Precision:0.692\n",
      "Recall:0.917\n",
      "===============Fold report==================\n",
      "Accuracy:0.628\n",
      "Precision:0.627\n",
      "Recall:0.999\n",
      "===============Fold report==================\n",
      "Accuracy:0.514\n",
      "Precision:0.942\n",
      "Recall:0.246\n",
      "===============Fold report==================\n",
      "Accuracy:0.513\n",
      "Precision:0.962\n",
      "Recall:0.220\n",
      "===============Fold report==================\n",
      "Accuracy:0.629\n",
      "Precision:0.629\n",
      "Recall:1.000\n",
      "===============Final report=================\n",
      "Accuracy:0.596 0.071\n",
      "Precision:0.770 0.150\n",
      "Recall:0.677 0.363\n",
      "Area under ROC 0.074\n",
      "===============Fold report==================\n",
      "Accuracy:0.640\n",
      "Precision:0.491\n",
      "Recall:0.859\n",
      "===============Fold report==================\n",
      "Accuracy:0.794\n",
      "Precision:0.655\n",
      "Recall:0.693\n",
      "===============Fold report==================\n",
      "Accuracy:0.767\n",
      "Precision:0.959\n",
      "Recall:0.315\n",
      "===============Fold report==================\n",
      "Accuracy:0.664\n",
      "Precision:0.492\n",
      "Recall:0.808\n",
      "===============Fold report==================\n",
      "Accuracy:0.816\n",
      "Precision:0.729\n",
      "Recall:0.678\n",
      "===============Final report=================\n",
      "Accuracy:0.736 0.071\n",
      "Precision:0.665 0.174\n",
      "Recall:0.671 0.190\n",
      "Area under ROC 0.126\n",
      "===============Fold report==================\n",
      "Accuracy:0.640\n",
      "Precision:0.491\n",
      "Recall:0.859\n",
      "===============Fold report==================\n",
      "Accuracy:0.794\n",
      "Precision:0.655\n",
      "Recall:0.693\n",
      "===============Fold report==================\n",
      "Accuracy:0.767\n",
      "Precision:0.959\n",
      "Recall:0.315\n",
      "===============Fold report==================\n",
      "Accuracy:0.664\n",
      "Precision:0.492\n",
      "Recall:0.808\n",
      "===============Fold report==================\n",
      "Accuracy:0.816\n",
      "Precision:0.729\n",
      "Recall:0.678\n",
      "===============Final report=================\n",
      "Accuracy:0.736 0.071\n",
      "Precision:0.665 0.174\n",
      "Recall:0.671 0.190\n",
      "Area under ROC 0.126\n",
      "===============Fold report==================\n",
      "Accuracy:0.640\n",
      "Precision:0.491\n",
      "Recall:0.859\n",
      "===============Fold report==================\n",
      "Accuracy:0.794\n",
      "Precision:0.655\n",
      "Recall:0.693\n",
      "===============Fold report==================\n",
      "Accuracy:0.767\n",
      "Precision:0.959\n",
      "Recall:0.315\n",
      "===============Fold report==================\n",
      "Accuracy:0.664\n",
      "Precision:0.492\n",
      "Recall:0.808\n",
      "===============Fold report==================\n",
      "Accuracy:0.816\n",
      "Precision:0.729\n",
      "Recall:0.678\n",
      "===============Final report=================\n",
      "Accuracy:0.736 0.071\n",
      "Precision:0.665 0.174\n",
      "Recall:0.671 0.190\n",
      "Area under ROC 0.126\n",
      "===============Fold report==================\n",
      "Accuracy:0.696\n",
      "Precision:0.742\n",
      "Recall:0.785\n",
      "===============Fold report==================\n",
      "Accuracy:0.697\n",
      "Precision:0.744\n",
      "Recall:0.787\n",
      "===============Fold report==================\n",
      "Accuracy:0.702\n",
      "Precision:0.748\n",
      "Recall:0.796\n",
      "===============Fold report==================\n",
      "Accuracy:0.699\n",
      "Precision:0.740\n",
      "Recall:0.792\n",
      "===============Fold report==================\n",
      "Accuracy:0.712\n",
      "Precision:0.756\n",
      "Recall:0.800\n",
      "===============Final report=================\n",
      "Accuracy:0.701 0.006\n",
      "Precision:0.746 0.006\n",
      "Recall:0.792 0.006\n",
      "Area under ROC 0.057\n",
      "===============Fold report==================\n",
      "Accuracy:0.698\n",
      "Precision:0.742\n",
      "Recall:0.790\n",
      "===============Fold report==================\n",
      "Accuracy:0.698\n",
      "Precision:0.743\n",
      "Recall:0.789\n",
      "===============Fold report==================\n",
      "Accuracy:0.702\n",
      "Precision:0.746\n",
      "Recall:0.801\n",
      "===============Fold report==================\n",
      "Accuracy:0.701\n",
      "Precision:0.739\n",
      "Recall:0.798\n",
      "===============Fold report==================\n",
      "Accuracy:0.712\n",
      "Precision:0.755\n",
      "Recall:0.803\n",
      "===============Final report=================\n",
      "Accuracy:0.702 0.005\n",
      "Precision:0.745 0.006\n",
      "Recall:0.796 0.006\n",
      "Area under ROC 0.056\n",
      "===============Fold report==================\n",
      "Accuracy:0.698\n",
      "Precision:0.742\n",
      "Recall:0.790\n",
      "===============Fold report==================\n",
      "Accuracy:0.698\n",
      "Precision:0.743\n",
      "Recall:0.789\n",
      "===============Fold report==================\n",
      "Accuracy:0.702\n",
      "Precision:0.746\n",
      "Recall:0.801\n",
      "===============Fold report==================\n",
      "Accuracy:0.701\n",
      "Precision:0.739\n",
      "Recall:0.798\n",
      "===============Fold report==================\n",
      "Accuracy:0.713\n",
      "Precision:0.755\n",
      "Recall:0.805\n",
      "===============Final report=================\n",
      "Accuracy:0.702 0.006\n",
      "Precision:0.745 0.006\n",
      "Recall:0.796 0.006\n",
      "Area under ROC 0.055\n",
      "===============Fold report==================\n",
      "Accuracy:0.638\n",
      "Precision:0.488\n",
      "Recall:0.801\n",
      "===============Fold report==================\n",
      "Accuracy:0.599\n",
      "Precision:0.417\n",
      "Recall:0.774\n",
      "===============Fold report==================\n",
      "Accuracy:0.650\n",
      "Precision:0.487\n",
      "Recall:0.866\n",
      "===============Fold report==================\n",
      "Accuracy:0.626\n",
      "Precision:0.458\n",
      "Recall:0.795\n",
      "===============Fold report==================\n",
      "Accuracy:0.643\n",
      "Precision:0.466\n",
      "Recall:0.762\n",
      "===============Final report=================\n",
      "Accuracy:0.631 0.018\n",
      "Precision:0.463 0.026\n",
      "Recall:0.800 0.036\n",
      "Area under ROC 0.055\n",
      "===============Fold report==================\n",
      "Accuracy:0.624\n",
      "Precision:0.478\n",
      "Recall:0.833\n",
      "===============Fold report==================\n",
      "Accuracy:0.590\n",
      "Precision:0.414\n",
      "Recall:0.810\n",
      "===============Fold report==================\n",
      "Accuracy:0.628\n",
      "Precision:0.470\n",
      "Recall:0.899\n",
      "===============Fold report==================\n",
      "Accuracy:0.603\n",
      "Precision:0.442\n",
      "Recall:0.815\n",
      "===============Fold report==================\n",
      "Accuracy:0.626\n",
      "Precision:0.453\n",
      "Recall:0.804\n",
      "===============Final report=================\n",
      "Accuracy:0.614 0.015\n",
      "Precision:0.451 0.022\n",
      "Recall:0.832 0.035\n",
      "Area under ROC 0.042\n",
      "===============Fold report==================\n",
      "Accuracy:0.622\n",
      "Precision:0.476\n",
      "Recall:0.833\n",
      "===============Fold report==================\n",
      "Accuracy:0.592\n",
      "Precision:0.416\n",
      "Recall:0.818\n",
      "===============Fold report==================\n",
      "Accuracy:0.628\n",
      "Precision:0.470\n",
      "Recall:0.899\n",
      "===============Fold report==================\n",
      "Accuracy:0.603\n",
      "Precision:0.442\n",
      "Recall:0.815\n",
      "===============Fold report==================\n",
      "Accuracy:0.626\n",
      "Precision:0.453\n",
      "Recall:0.804\n",
      "===============Final report=================\n",
      "Accuracy:0.614 0.014\n",
      "Precision:0.452 0.021\n",
      "Recall:0.834 0.034\n",
      "Area under ROC 0.042\n"
     ]
    }
   ],
   "source": [
    "spam_logreg = []\n",
    "volcanoes_logreg = []\n",
    "spam_nbayes = []\n",
    "volcanoes_nbayes = []\n",
    "random.seed(12345)\n",
    "spam_logreg.append(boost(\"440data/spam\", 0, \"logreg\", 2)) \n",
    "random.seed(12345)\n",
    "spam_logreg.append(boost(\"440data/spam\", 0, \"logreg\", 10))\n",
    "random.seed(12345)\n",
    "spam_logreg.append(boost(\"440data/spam\", 0, \"logreg\", 50))\n",
    "random.seed(12345)\n",
    "volcanoes_logreg.append(boost(\"440data/volcanoes\", 0, \"logreg\", 2))\n",
    "random.seed(12345)\n",
    "volcanoes_logreg.append(boost(\"440data/volcanoes\", 0, \"logreg\", 10))\n",
    "random.seed(12345)\n",
    "volcanoes_logreg.append(boost(\"440data/volcanoes\", 0, \"logreg\", 50))\n",
    "random.seed(12345)\n",
    "spam_nbayes.append(boost(\"440data/spam\", 0, \"nbayes\", 2))\n",
    "random.seed(12345)\n",
    "spam_nbayes.append(boost(\"440data/spam\", 0, \"nbayes\", 10))\n",
    "random.seed(12345)\n",
    "spam_nbayes.append(boost(\"440data/spam\", 0, \"nbayes\", 50))\n",
    "random.seed(12345)\n",
    "volcanoes_nbayes.append(boost(\"440data/volcanoes\", 0, \"nbayes\", 2))\n",
    "random.seed(12345)\n",
    "volcanoes_nbayes.append(boost(\"440data/volcanoes\", 0, \"nbayes\", 10))\n",
    "random.seed(12345)\n",
    "volcanoes_nbayes.append(boost(\"440data/volcanoes\", 0, \"nbayes\", 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 21516149,
     "status": "ok",
     "timestamp": 1604904257491,
     "user": {
      "displayName": "Xin Yao",
      "photoUrl": "",
      "userId": "04992890314986629859"
     },
     "user_tz": -480
    },
    "id": "1bFTUpFB0iqD",
    "outputId": "1f9753b1-f01b-4595-9914-08a111454067"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============Fold report==================\n",
      "Accuracy:1.000\n",
      "Precision:1.000\n",
      "Recall:1.000\n",
      "===============Fold report==================\n",
      "Accuracy:0.989\n",
      "Precision:0.980\n",
      "Recall:1.000\n",
      "===============Fold report==================\n",
      "Accuracy:0.989\n",
      "Precision:0.980\n",
      "Recall:1.000\n",
      "===============Fold report==================\n",
      "Accuracy:0.989\n",
      "Precision:0.980\n",
      "Recall:1.000\n",
      "===============Fold report==================\n",
      "Accuracy:1.000\n",
      "Precision:1.000\n",
      "Recall:1.000\n",
      "===============Final report=================\n",
      "Accuracy:0.993 0.006\n",
      "Precision:0.988 0.010\n",
      "Recall:1.000 0.000\n",
      "Area under ROC 0.000\n",
      "===============Fold report==================\n",
      "Accuracy:1.000\n",
      "Precision:1.000\n",
      "Recall:1.000\n",
      "===============Fold report==================\n",
      "Accuracy:0.989\n",
      "Precision:0.980\n",
      "Recall:1.000\n",
      "===============Fold report==================\n",
      "Accuracy:0.989\n",
      "Precision:0.980\n",
      "Recall:1.000\n",
      "===============Fold report==================\n",
      "Accuracy:0.989\n",
      "Precision:0.980\n",
      "Recall:1.000\n",
      "===============Fold report==================\n",
      "Accuracy:1.000\n",
      "Precision:1.000\n",
      "Recall:1.000\n",
      "===============Final report=================\n",
      "Accuracy:0.993 0.006\n",
      "Precision:0.988 0.010\n",
      "Recall:1.000 0.000\n",
      "Area under ROC 0.000\n",
      "===============Fold report==================\n",
      "Accuracy:1.000\n",
      "Precision:1.000\n",
      "Recall:1.000\n",
      "===============Fold report==================\n",
      "Accuracy:0.989\n",
      "Precision:0.980\n",
      "Recall:1.000\n",
      "===============Fold report==================\n",
      "Accuracy:0.989\n",
      "Precision:0.980\n",
      "Recall:1.000\n",
      "===============Fold report==================\n",
      "Accuracy:0.989\n",
      "Precision:0.980\n",
      "Recall:1.000\n",
      "===============Fold report==================\n",
      "Accuracy:1.000\n",
      "Precision:1.000\n",
      "Recall:1.000\n",
      "===============Final report=================\n",
      "Accuracy:0.993 0.006\n",
      "Precision:0.988 0.010\n",
      "Recall:1.000 0.000\n",
      "Area under ROC 0.000\n",
      "===============Fold report==================\n",
      "Accuracy:0.673\n",
      "Precision:0.673\n",
      "Recall:1.000\n",
      "===============Fold report==================\n",
      "Accuracy:0.601\n",
      "Precision:0.651\n",
      "Recall:0.877\n",
      "===============Fold report==================\n",
      "Accuracy:0.578\n",
      "Precision:0.642\n",
      "Recall:0.843\n",
      "===============Fold report==================\n",
      "Accuracy:0.675\n",
      "Precision:0.711\n",
      "Recall:0.870\n",
      "===============Fold report==================\n",
      "Accuracy:0.673\n",
      "Precision:0.698\n",
      "Recall:0.903\n",
      "===============Final report=================\n",
      "Accuracy:0.640 0.042\n",
      "Precision:0.675 0.027\n",
      "Recall:0.899 0.054\n",
      "Area under ROC 0.005\n",
      "===============Fold report==================\n",
      "Accuracy:0.673\n",
      "Precision:0.673\n",
      "Recall:1.000\n",
      "===============Fold report==================\n",
      "Accuracy:0.601\n",
      "Precision:0.651\n",
      "Recall:0.877\n",
      "===============Fold report==================\n",
      "Accuracy:0.578\n",
      "Precision:0.642\n",
      "Recall:0.843\n",
      "===============Fold report==================\n",
      "Accuracy:0.675\n",
      "Precision:0.711\n",
      "Recall:0.870\n",
      "===============Fold report==================\n",
      "Accuracy:0.673\n",
      "Precision:0.698\n",
      "Recall:0.903\n",
      "===============Final report=================\n",
      "Accuracy:0.640 0.042\n",
      "Precision:0.675 0.027\n",
      "Recall:0.899 0.054\n",
      "Area under ROC 0.005\n",
      "===============Fold report==================\n",
      "Accuracy:0.673\n",
      "Precision:0.673\n",
      "Recall:1.000\n",
      "===============Fold report==================\n",
      "Accuracy:0.601\n",
      "Precision:0.651\n",
      "Recall:0.877\n",
      "===============Fold report==================\n",
      "Accuracy:0.578\n",
      "Precision:0.642\n",
      "Recall:0.843\n",
      "===============Fold report==================\n",
      "Accuracy:0.675\n",
      "Precision:0.711\n",
      "Recall:0.870\n",
      "===============Fold report==================\n",
      "Accuracy:0.673\n",
      "Precision:0.698\n",
      "Recall:0.903\n",
      "===============Final report=================\n",
      "Accuracy:0.640 0.042\n",
      "Precision:0.675 0.027\n",
      "Recall:0.899 0.054\n",
      "Area under ROC 0.005\n"
     ]
    }
   ],
   "source": [
    "spam_dtree = []\n",
    "volcanoes_dtree = []\n",
    "random.seed(12345)\n",
    "spam_dtree.append(boost(\"440data/voting\", 0, \"dtree\", 2))\n",
    "random.seed(12345)\n",
    "spam_dtree.append(boost(\"440data/voting\", 0, \"dtree\", 10))\n",
    "random.seed(12345)\n",
    "spam_dtree.append(boost(\"440data/voting\", 0, \"dtree\", 50))\n",
    "random.seed(12345)\n",
    "volcanoes_dtree.append(boost(\"440data/volcanoes\", 0, \"dtree\", 2))\n",
    "random.seed(12345)\n",
    "volcanoes_dtree.append(boost(\"440data/volcanoes\", 0, \"dtree\", 10))\n",
    "random.seed(12345)\n",
    "volcanoes_dtree.append(boost(\"440data/volcanoes\", 0, \"dtree\", 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21541899,
     "status": "ok",
     "timestamp": 1604904286622,
     "user": {
      "displayName": "Xin Yao",
      "photoUrl": "",
      "userId": "04992890314986629859"
     },
     "user_tz": -480
    },
    "id": "xlWH02km1Ygc",
    "outputId": "5d6ec92b-9788-4492-bd00-7ad7d513262b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.5957356730557685, 0.7704861770220492, 0.6765340054950781, 0.07431591111897175), (0.5957356730557685, 0.7704861770220492, 0.6765340054950781, 0.07431591111897175), (0.5957356730557685, 0.7704861770220492, 0.6765340054950781, 0.07431591111897175)]\n",
      "[(0.7360359546954787, 0.6652377106084651, 0.6708764227846549, 0.1259644322845417), (0.7360359546954787, 0.6652377106084651, 0.6708764227846549, 0.1259644322845417), (0.7360359546954787, 0.6652377106084651, 0.6708764227846549, 0.1259644322845417)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.7012551529184226, 0.7460064008284102, 0.7918641378382423, 0.057226095652341484), (0.7020981081687989, 0.7450640331377569, 0.7959760579675513, 0.05562652616858648), (0.7022452948952322, 0.7450260632350083, 0.7964013184395904, 0.05547712627280872)]\n",
      "[(0.6311042224696782, 0.4633413054006478, 0.7995069602565564, 0.05479160966712266), (0.6140698829265356, 0.45148616144252723, 0.832429093568483, 0.04233561331509348), (0.6140708861267443, 0.4515715759556665, 0.8338889475830815, 0.041933424532603736)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.9931034482758619, 0.9877551020408163, 1.0, 0.0), (0.9931034482758619, 0.9877551020408163, 1.0, 0.0), (0.9931034482758619, 0.9877551020408163, 1.0, 0.0)]\n",
      "[(0.6399103139013453, 0.6750786177976819, 0.8986666666666668, 0.00548310502283105), (0.6399103139013453, 0.6750786177976819, 0.8986666666666668, 0.00548310502283105), (0.6399103139013453, 0.6750786177976819, 0.8986666666666668, 0.00548310502283105)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# b. Pick at least three iteration values between 2 and 50\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "x_label = [2, 10, 50]\n",
    "\n",
    "print(spam_logreg)\n",
    "print(volcanoes_logreg)\n",
    "acc_spam = list(map(lambda x:x[0], spam_logreg))\n",
    "acc_volcanoes = list(map(lambda x:x[0], volcanoes_logreg))\n",
    "plt.figure()\n",
    "plt.plot(x_label, acc_spam, c='red', label = \"spam\")\n",
    "plt.plot(x_label, acc_volcanoes, c='blue', label = \"volcanoes\")\n",
    "plt.ylim(0.3, 1)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(spam_nbayes)\n",
    "print(volcanoes_nbayes)\n",
    "acc_spam = list(map(lambda x:x[0], spam_nbayes))\n",
    "acc_volcanoes = list(map(lambda x:x[0], volcanoes_nbayes))\n",
    "plt.figure()\n",
    "plt.plot(x_label, acc_spam, c='red', label = \"spam\")\n",
    "plt.plot(x_label, acc_volcanoes, c='blue', label = \"volcanoes\")\n",
    "plt.ylim(0.3, 1)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(spam_dtree)\n",
    "print(volcanoes_dtree)\n",
    "acc_spam = list(map(lambda x:x[0], spam_dtree))\n",
    "acc_volcanoes = list(map(lambda x:x[0], volcanoes_dtree))\n",
    "plt.figure()\n",
    "plt.plot(x_label, acc_spam, c='red', label = \"voting\")\n",
    "plt.plot(x_label, acc_volcanoes, c='blue', label = \"volcanoes\")\n",
    "plt.ylim(0.3, 1.3)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21539174,
     "status": "ok",
     "timestamp": 1604904286626,
     "user": {
      "displayName": "Xin Yao",
      "photoUrl": "",
      "userId": "04992890314986629859"
     },
     "user_tz": -480
    },
    "id": "D-5C0lU6_C2T",
    "outputId": "ab7cfb3e-da08-40a6-b71a-e3e659b83a6b"
   },
   "source": [
    "(a) When compared to our pervious results of using the base learner, \n",
    "1. dtree\n",
    "   - boosting does not improve the accuracy in voting and volcanoes dataset. Maybe these two datasets are small.\n",
    "2. nbayes\n",
    "   - boosting increases the accuracy from 63.2% to 63.9% on volcanoes dataset, while the accuracy reduced from 70.8% to 70.2% on spam dataset. The precision increases from 74.4% to 74.6% on spam dataset.\n",
    "3. logreg\n",
    "   - boosting increases the accuracy from 73.5% to 73.6% on volcanoes dataset, while the accuracy reduced from 60.5% to 59.6% on spam dataset. The recall increases from 60.1% to 67.7% on spam dataset.\n",
    "\n",
    "(b) We found the number of itertaions does not take much efferts on improving the accuracy. Although we have observed **decreasing epsilon** with increasing iterations, it seems the accuracy remains.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============Fold report==================\n",
      "Accuracy:0.638\n",
      "Precision:0.488\n",
      "Recall:0.801\n",
      "===============Fold report==================\n",
      "Accuracy:0.601\n",
      "Precision:0.419\n",
      "Recall:0.774\n",
      "===============Fold report==================\n",
      "Accuracy:0.650\n",
      "Precision:0.487\n",
      "Recall:0.866\n",
      "===============Fold report==================\n",
      "Accuracy:0.626\n",
      "Precision:0.458\n",
      "Recall:0.795\n",
      "===============Fold report==================\n",
      "Accuracy:0.643\n",
      "Precision:0.466\n",
      "Recall:0.762\n",
      "===============Final report=================\n",
      "Accuracy:0.632 0.017\n",
      "Precision:0.464 0.025\n",
      "Recall:0.800 0.036\n",
      "Area under ROC 0.055\n",
      "===============Fold report==================\n",
      "Accuracy:0.642\n",
      "Precision:0.486\n",
      "Recall:0.810\n",
      "===============Fold report==================\n",
      "Accuracy:0.626\n",
      "Precision:0.458\n",
      "Recall:0.861\n",
      "===============Fold report==================\n",
      "Accuracy:0.601\n",
      "Precision:0.437\n",
      "Recall:0.735\n",
      "===============Fold report==================\n",
      "Accuracy:0.650\n",
      "Precision:0.463\n",
      "Recall:0.812\n",
      "===============Fold report==================\n",
      "Accuracy:0.632\n",
      "Precision:0.470\n",
      "Recall:0.792\n",
      "===============Final report=================\n",
      "Accuracy:0.630 0.017\n",
      "Precision:0.463 0.016\n",
      "Recall:0.802 0.041\n",
      "Area under ROC 0.054\n",
      "===============Fold report==================\n",
      "Accuracy:0.613\n",
      "Precision:0.442\n",
      "Recall:0.838\n",
      "===============Fold report==================\n",
      "Accuracy:0.594\n",
      "Precision:0.421\n",
      "Recall:0.826\n",
      "===============Fold report==================\n",
      "Accuracy:0.596\n",
      "Precision:0.437\n",
      "Recall:0.808\n",
      "===============Fold report==================\n",
      "Accuracy:0.639\n",
      "Precision:0.457\n",
      "Recall:0.835\n",
      "===============Fold report==================\n",
      "Accuracy:0.623\n",
      "Precision:0.497\n",
      "Recall:0.855\n",
      "===============Final report=================\n",
      "Accuracy:0.613 0.017\n",
      "Precision:0.451 0.026\n",
      "Recall:0.832 0.015\n",
      "Area under ROC 0.042\n"
     ]
    }
   ],
   "source": [
    "def boost(path, option, solver_type, num_iters, input_epsilon): \n",
    "    path = path.replace(\"\\\\\", \"/\")\n",
    "    file_base = path.split('/')[-1]\n",
    "    rootdir=path\n",
    "\n",
    "    epsilon_thread = input_epsilon\n",
    "\n",
    "    data = mldata.parse_c45(file_base, rootdir)\n",
    "    n_bin = 1\n",
    "    cross_validation = False\n",
    "    if option == 0:\n",
    "        n_bin = 5\n",
    "        cross_validation = True\n",
    "\n",
    "    data = np.asarray(data.to_float())\n",
    "    X_data = data[:, 1:-1]\n",
    "    X_data = preprocess.process(X_data, file_base, n_bin)\n",
    "    y_data = data[:, -1].astype(int)\n",
    "    # print(len(X_data))\n",
    "    # partition the data into multiple dataset,\n",
    "    folds = util.n_fold(len(data), n_bin)\n",
    "\n",
    "    # nbayes:\n",
    "    posi_num = [{} for i in range(len(X_data[0]))]\n",
    "    nega_num = [{} for i in range(len(X_data[0]))]\n",
    "            \n",
    "    for i, d in enumerate(posi_num):\n",
    "        for attr in np.unique(X_data[:, i]):\n",
    "            posi_num[i][attr] = 0\n",
    "                    \n",
    "    for i, d in enumerate(nega_num):\n",
    "        for attr in np.unique(X_data[:, i]):\n",
    "            nega_num[i][attr] = 0\n",
    "\n",
    "    AUC_y = []\n",
    "    pred_AUC_y = []\n",
    "    acc = []\n",
    "    prec = []\n",
    "    rec = []\n",
    "    # training and evaluating\n",
    "                \n",
    "    for i in range(n_bin):\n",
    "        if solver_type == \"dtree\":\n",
    "            tree = ID3DecisionTree(1, path, \"gain\", cross_validation)\n",
    "            x_train, y_train, x_test, y_test = tree.create_for_train(n_bin, i)\n",
    "            train_size = len(x_train)\n",
    "            wboost = np.ones((train_size, 1)).astype(float)/train_size\n",
    "            alphas = []\n",
    "            epsilons = []\n",
    "            forest = []\n",
    "            for iter_ in range(num_iters): \n",
    "                tree = ID3DecisionTree(1, path, \"gain\", cross_validation)\n",
    "                x_train, y_train, x_test, y_test = tree.create_for_train(n_bin, i)\n",
    "                D_train = (x_train, y_train)\n",
    "                wboost, epsilon, alpha = tree.boosttrain(D_train, wboost, epsilon_thread)\n",
    "                forest.append(tree)\n",
    "                epsilons.append(epsilon)\n",
    "                if epsilon == 0:\n",
    "                    alphas = [0] * len(alphas)\n",
    "                    alphas.append(1)\n",
    "                    break\n",
    "                elif epsilon <= epsilon_thread or epsilon >= 0.5:\n",
    "                    alphas.append(alpha)\n",
    "                    break\n",
    "                else:\n",
    "                    alphas.append(alpha)\n",
    "                # y_pred = tree.test(x_test)\n",
    "            result = []\n",
    "            for i in range(len(forest)):\n",
    "                y_predB = forest[i].test(x_test)\n",
    "                y_pred = np.array(y_predB)\n",
    "                y_pred[y_pred==False] = 0\n",
    "                y_pred[y_pred==True] = 1\n",
    "                result.append(y_pred)\n",
    "            alphas = np.array(alphas)\n",
    "            alphas = alphas/np.sum(alphas)\n",
    "            y_pred = alphas.dot(np.array(result))\n",
    "            y_pred[y_pred<0.5] = 0\n",
    "            y_pred[y_pred>=0.5] = 1\n",
    "            y_test = np.array(y_test)\n",
    "            y_test[y_test<0.5] = 0\n",
    "            y_test[y_test>=0.5] = 1\n",
    "            AUC_y.extend(y_test)\n",
    "            pred_AUC_y.extend(y_pred)\n",
    "            _acc, _prec, _rec = util.cal_APR(y_pred, y_test)\n",
    "            if cross_validation:\n",
    "                util.report_cross(_acc, _prec, _rec)\n",
    "            acc.append(_acc)\n",
    "            prec.append(_prec)\n",
    "            rec.append(_rec)\n",
    "        elif solver_type == \"nbayes\":\n",
    "            m_etimate = 0.1\n",
    "            x_train, y_train, x_test, y_test = create_for_train(X_data, y_data, folds, n_bin, i)\n",
    "            train_size = len(x_train)\n",
    "            wboost = np.ones((train_size, 1)).astype(float)/train_size\n",
    "            alphas = []\n",
    "            epsilons = []\n",
    "            pre_ps = []\n",
    "            posi_ps = []\n",
    "            nega_ps = []\n",
    "            for iter_ in range(num_iters): \n",
    "                pre_p, posi_p, nega_p, epsilon, alpha, wboost = nbayes.boosttrain_bayes(x_train, y_train, m_etimate, posi_num, nega_num, wboost, epsilon_thread)\n",
    "                epsilons.append(epsilon)\n",
    "                pre_ps.append(pre_p)\n",
    "                posi_ps.append(posi_p)\n",
    "                nega_ps.append(nega_p)\n",
    "                if epsilon == 0:\n",
    "                    alphas = [0] * len(alphas)\n",
    "                    alphas.append(1)\n",
    "                    break\n",
    "                elif epsilon <= epsilon_thread or epsilon >= 0.5:\n",
    "                    alphas.append(alpha)\n",
    "                    break\n",
    "                else:\n",
    "                    alphas.append(alpha)\n",
    "            result = []\n",
    "            for i in range(len(pre_ps)):\n",
    "                y_predB = nbayes.pred(x_test, pre_ps[i], posi_ps[i], nega_ps[i])\n",
    "                y_pred = []\n",
    "                for i in y_predB:\n",
    "                    if i[0] > i[1]:\n",
    "                        y_pred.append(0)\n",
    "                    else:\n",
    "                        y_pred.append(1)\n",
    "                y_pred = np.array(y_pred)\n",
    "                result.append(y_pred)\n",
    "            alphas = np.array(alphas)\n",
    "            alphas = alphas/np.sum(alphas)\n",
    "            y_pred = alphas.dot(np.array(result))\n",
    "            y_pred[y_pred<0.5] = 0\n",
    "            y_pred[y_pred>=0.5] = 1\n",
    "            AUC_y.extend(y_test)\n",
    "            pred_AUC_y.extend(y_pred)\n",
    "            _acc, _prec, _rec = util.cal_APR(y_pred, y_test)\n",
    "            if cross_validation:\n",
    "                util.report_cross(_acc, _prec, _rec)\n",
    "            acc.append(_acc)\n",
    "            prec.append(_prec)\n",
    "            rec.append(_rec)\n",
    "        elif solver_type == \"logreg\":\n",
    "            # train\n",
    "            x_train, y_train, x_test, y_test = create_for_train(X_data, y_data, folds, n_bin, i)\n",
    "            train_size = len(x_train)\n",
    "            wboost = np.ones((train_size, 1)).astype(float)/train_size\n",
    "            alphas = []\n",
    "            epsilons = []\n",
    "            weights = []\n",
    "            for iter_ in range(num_iters): \n",
    "                weight, epsilon, alpha, wboost = logreg.boostLR(x_train, y_train, wboost, epsilon_thread, max_iters=500, lbd=0.1)\n",
    "                epsilons.append(epsilon)\n",
    "                weights.append(weight)\n",
    "                if epsilon == 0:\n",
    "                    alphas = [0] * len(alphas)\n",
    "                    alphas.append(1)\n",
    "                    break\n",
    "                elif epsilon <= epsilon_thread or epsilon >= 0.5:\n",
    "                    alphas.append(alpha)\n",
    "                    break\n",
    "                else:\n",
    "                    alphas.append(alpha)\n",
    "            result = []\n",
    "            for i in range(len(weights)):\n",
    "                result.append(logreg.pred(x_test, weights[i]))\n",
    "            alphas = np.array(alphas)\n",
    "            alphas = alphas/np.sum(alphas)\n",
    "            y_pred = alphas.dot(np.array(result))\n",
    "            y_pred[y_pred<0.5] = 0\n",
    "            y_pred[y_pred>=0.5] = 1\n",
    "            AUC_y.extend(y_test)\n",
    "            pred_AUC_y.extend(y_pred)\n",
    "            _acc, _prec, _rec = logreg.cal_LR_APR(y_pred, y_test)\n",
    "            if cross_validation:\n",
    "                util.report_cross(_acc, _prec, _rec)\n",
    "            acc.append(_acc)\n",
    "            prec.append(_prec)\n",
    "            rec.append(_rec)\n",
    "        else:\n",
    "            return \n",
    "    roc_score = logreg.cal_AUC(AUC_y, pred_AUC_y)\n",
    "    util.report(acc, prec, rec, roc_score)\n",
    "    if type(acc) is list:\n",
    "        return np.mean(acc), np.mean(prec), np.mean(rec), roc_score\n",
    "    return acc, prec, rec, roc_score\n",
    "\n",
    "research_ext = []\n",
    "\n",
    "random.seed(12345)\n",
    "research_ext.append(boost(\"440data/volcanoes\", 0, \"nbayes\", 50, 0.3))\n",
    "research_ext.append(boost(\"440data/volcanoes\", 0, \"nbayes\", 50, 0.1))\n",
    "research_ext.append(boost(\"440data/volcanoes\", 0, \"nbayes\", 50, 0.00000001))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.6315526529629517, 0.46367120487467595, 0.7995069602565564, 0.05485818513451892), (0.6302053550827139, 0.4628030976560307, 0.8019606033250767, 0.05421796625626995), (0.6131780379410319, 0.45065536714491217, 0.8324576728931448, 0.04222435020519836)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(research_ext)\n",
    "x_label = [0.3, 0.1, 0.00000001]\n",
    "plt.figure()\n",
    "research_acc = list(map(lambda x:x[0], research_ext))\n",
    "plt.plot(range(3), research_acc, c='red', label = \"spam\")\n",
    "plt.xticks(range(3), x_label)\n",
    "plt.ylim(0.3, 1)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Research extension\n",
    "\n",
    "We change the epsilon threadholds: 0.3, 0.1, and 0.00000001 to see whether epsilon threadhold can affect the accuracy.\n",
    "\n",
    "We found the accuracy dropped when the threhold of the epsilon is small (1e-8), so searching for an adaptive epsilon is very important to acquire a good accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "p4_part3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
